[
    {
        "label": "PyPDF2",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "PyPDF2",
        "description": "PyPDF2",
        "detail": "PyPDF2",
        "documentation": {}
    },
    {
        "label": "HuggingFaceEmbeddings",
        "importPath": "langchain_community.embeddings",
        "description": "langchain_community.embeddings",
        "isExtraImport": true,
        "detail": "langchain_community.embeddings",
        "documentation": {}
    },
    {
        "label": "for Chroma",
        "importPath": "langchain_chroma import Chroma  # Updated",
        "description": "langchain_chroma import Chroma  # Updated",
        "isExtraImport": true,
        "detail": "langchain_chroma import Chroma  # Updated",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "signal",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "signal",
        "description": "signal",
        "detail": "signal",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "HuggingFaceEmbeddings",
        "importPath": "langchain_huggingface",
        "description": "langchain_huggingface",
        "isExtraImport": true,
        "detail": "langchain_huggingface",
        "documentation": {}
    },
    {
        "label": "Chroma",
        "importPath": "langchain_chroma",
        "description": "langchain_chroma",
        "isExtraImport": true,
        "detail": "langchain_chroma",
        "documentation": {}
    },
    {
        "label": "google.generativeai",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "google.generativeai",
        "description": "google.generativeai",
        "detail": "google.generativeai",
        "documentation": {}
    },
    {
        "label": "embed_data",
        "kind": 2,
        "importPath": "Import",
        "description": "Import",
        "peekOfCode": "def embed_data(data, embeddings):\n    return embeddings.embed_documents(data)  \n# Instantiate the HuggingFaceEmbeddings model\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n# Embed the extracted PDF text\nembedded_texts = embed_data(all_text, embeddings)\n# # Print each embedded vector\n# for i, vector in enumerate(embedded_texts):\n#     print(f\"Vector for Page {i + 1}: {vector}\\n\")\n# Initialize the Chroma vector store with the embeddings and original text",
        "detail": "Import",
        "documentation": {}
    },
    {
        "label": "ExampleP",
        "kind": 5,
        "importPath": "Import",
        "description": "Import",
        "peekOfCode": "ExampleP = 'SE-Lesson1.pdf'\nall_text = []\nwith open(ExampleP, 'rb') as pdffile:\n    reader = PyPDF2.PdfReader(pdffile)\n    # Extract text from each page and store in a list\n    for pagenum in range(len(reader.pages)):\n        page = reader.pages[pagenum]\n        text = page.extract_text()\n        all_text.append(text)  \n# Embedding function to embed the data",
        "detail": "Import",
        "documentation": {}
    },
    {
        "label": "all_text",
        "kind": 5,
        "importPath": "Import",
        "description": "Import",
        "peekOfCode": "all_text = []\nwith open(ExampleP, 'rb') as pdffile:\n    reader = PyPDF2.PdfReader(pdffile)\n    # Extract text from each page and store in a list\n    for pagenum in range(len(reader.pages)):\n        page = reader.pages[pagenum]\n        text = page.extract_text()\n        all_text.append(text)  \n# Embedding function to embed the data\ndef embed_data(data, embeddings):",
        "detail": "Import",
        "documentation": {}
    },
    {
        "label": "embeddings",
        "kind": 5,
        "importPath": "Import",
        "description": "Import",
        "peekOfCode": "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n# Embed the extracted PDF text\nembedded_texts = embed_data(all_text, embeddings)\n# # Print each embedded vector\n# for i, vector in enumerate(embedded_texts):\n#     print(f\"Vector for Page {i + 1}: {vector}\\n\")\n# Initialize the Chroma vector store with the embeddings and original text\nvector_store = Chroma.from_texts(texts=all_text, embedding=embeddings, persist_directory=\"AssignmentDB\")\n# Use the correct method to interact with the Chroma vector store\ndocument_count = vector_store._collection.count()",
        "detail": "Import",
        "documentation": {}
    },
    {
        "label": "embedded_texts",
        "kind": 5,
        "importPath": "Import",
        "description": "Import",
        "peekOfCode": "embedded_texts = embed_data(all_text, embeddings)\n# # Print each embedded vector\n# for i, vector in enumerate(embedded_texts):\n#     print(f\"Vector for Page {i + 1}: {vector}\\n\")\n# Initialize the Chroma vector store with the embeddings and original text\nvector_store = Chroma.from_texts(texts=all_text, embedding=embeddings, persist_directory=\"AssignmentDB\")\n# Use the correct method to interact with the Chroma vector store\ndocument_count = vector_store._collection.count()\nprint(f\"Total number of documents stored: {document_count}\")\nprint(\"Vector store initialized and stored in 'AssignmentDB' directory.\")",
        "detail": "Import",
        "documentation": {}
    },
    {
        "label": "vector_store",
        "kind": 5,
        "importPath": "Import",
        "description": "Import",
        "peekOfCode": "vector_store = Chroma.from_texts(texts=all_text, embedding=embeddings, persist_directory=\"AssignmentDB\")\n# Use the correct method to interact with the Chroma vector store\ndocument_count = vector_store._collection.count()\nprint(f\"Total number of documents stored: {document_count}\")\nprint(\"Vector store initialized and stored in 'AssignmentDB' directory.\")",
        "detail": "Import",
        "documentation": {}
    },
    {
        "label": "document_count",
        "kind": 5,
        "importPath": "Import",
        "description": "Import",
        "peekOfCode": "document_count = vector_store._collection.count()\nprint(f\"Total number of documents stored: {document_count}\")\nprint(\"Vector store initialized and stored in 'AssignmentDB' directory.\")",
        "detail": "Import",
        "documentation": {}
    },
    {
        "label": "QueryResult",
        "kind": 6,
        "importPath": "Output",
        "description": "Output",
        "peekOfCode": "class QueryResult(BaseModel):\n    query: str\n# Define prompt generation function\ndef generate_rag_prompt(query, context):\n    escaped_context = context.replace(\"'\", \" \").replace('\"', ' ').replace('\\n', ' ')\n    prompt = f\"\"\"\nBased on the extracted information, answer the following question directly and in a focused manner:\nQuestion: {query}\nAnswer:\n    \"\"\"",
        "detail": "Output",
        "documentation": {}
    },
    {
        "label": "signal_handler",
        "kind": 2,
        "importPath": "Output",
        "description": "Output",
        "peekOfCode": "def signal_handler(sig, frame):\n    print('You pressed Ctrl+C! Exiting program.')\n    sys.exit(0)\nsignal.signal(signal.SIGINT, signal_handler)\n# Define data model for query result\nclass QueryResult(BaseModel):\n    query: str\n# Define prompt generation function\ndef generate_rag_prompt(query, context):\n    escaped_context = context.replace(\"'\", \" \").replace('\"', ' ').replace('\\n', ' ')",
        "detail": "Output",
        "documentation": {}
    },
    {
        "label": "generate_rag_prompt",
        "kind": 2,
        "importPath": "Output",
        "description": "Output",
        "peekOfCode": "def generate_rag_prompt(query, context):\n    escaped_context = context.replace(\"'\", \" \").replace('\"', ' ').replace('\\n', ' ')\n    prompt = f\"\"\"\nBased on the extracted information, answer the following question directly and in a focused manner:\nQuestion: {query}\nAnswer:\n    \"\"\"\n    return prompt\n# Define a function to embed and store initial data for queries\ndef initialize_vector_db(texts):",
        "detail": "Output",
        "documentation": {}
    },
    {
        "label": "initialize_vector_db",
        "kind": 2,
        "importPath": "Output",
        "description": "Output",
        "peekOfCode": "def initialize_vector_db(texts):\n    if not texts:\n        print(\"Error: No texts provided to initialize the vector database.\")\n        sys.exit(1)\n    try:\n        embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n        vector_db = Chroma.from_texts(texts=texts, embedding=embeddings, persist_directory=\"./AssignmentDB\")\n        return vector_db\n    except Exception as e:\n        print(f\"Error initializing vector database: {e}\")",
        "detail": "Output",
        "documentation": {}
    },
    {
        "label": "get_relative_context_fromDB",
        "kind": 2,
        "importPath": "Output",
        "description": "Output",
        "peekOfCode": "def get_relative_context_fromDB(query, vector_db):\n    context = \"\"\n    try:\n        search_results = vector_db.similarity_search(query, k=6)\n        for result in search_results:\n            context += result.page_content + \"\\n\"\n    except Exception as e:\n        print(f\"Error retrieving context from database: {e}\")\n    return context\n# Generate an answer based on the prompt",
        "detail": "Output",
        "documentation": {}
    },
    {
        "label": "generate_answer",
        "kind": 2,
        "importPath": "Output",
        "description": "Output",
        "peekOfCode": "def generate_answer(prompt):\n    try:\n        model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")\n        response = model.generate_content(prompt)\n        return response.text.strip() if response and response.text else \"No response generated.\"\n    except Exception as e:\n        return f\"Error generating answer: {e}\"\n# Main program loop\nif __name__ == \"__main__\":\n    # Prompt the user for a query",
        "detail": "Output",
        "documentation": {}
    },
    {
        "label": "Google_API_key",
        "kind": 5,
        "importPath": "Output",
        "description": "Output",
        "peekOfCode": "Google_API_key = os.getenv(\"GEMINI_API\")\nif Google_API_key:\n    genai.configure(api_key=Google_API_key)\nelse:\n    print(\"Error: Google API key not found. Please set GEMINI_API in the environment.\")\n    sys.exit(1)\n# Signal handler for clean exit\ndef signal_handler(sig, frame):\n    print('You pressed Ctrl+C! Exiting program.')\n    sys.exit(0)",
        "detail": "Output",
        "documentation": {}
    }
]